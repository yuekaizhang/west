# Copyright (c) 2025 Binbin Zhang(binbzha@qq.com)

from typing import Optional

import s3tokenizer
import torch
from transformers import (AutoConfig, AutoModelForCausalLM, AutoTokenizer,
                          GenerationMixin, PreTrainedModel)

from .configuration_touch_tts import TouchTTSConfig


class TouchTTS(PreTrainedModel, GenerationMixin):
    """ LLM based TTS, text in, speech token out
    """
    model_type = 'touch_tts'
    config_class = TouchTTSConfig
    supports_gradient_checkpointing = True

    def __init__(self, config: TouchTTSConfig):
        super().__init__(config)
        llm_config = AutoConfig.from_pretrained(config.llm_model_name_or_path)
        self.llm = AutoModelForCausalLM.from_pretrained(
            config.llm_model_name_or_path,
            config=llm_config,
            torch_dtype='auto',
            attn_implementation="flash_attention_2",  # or "flex_attention"
        )
        config.hidden_size = llm_config.hidden_size  # for deepseed training
        self.speech_tokenizer = s3tokenizer.load_model(
            config.s3tokenizer_model_name, config.s3tokenizer_download_dir)
        self.speech_tokenizer.freeze()
        # We assume the last num_speech_tokens units are speech tokens
        self.speech_code_start_idx = llm_config.vocab_size - config.num_speech_tokens

    def tie_weights(self):
        return self.llm.tie_weights()

    def reorg_ids(
        self,
        input_ids: torch.LongTensor = None,
        labels: Optional[torch.LongTensor] = None,
        audio_offsets: Optional[torch.LongTensor] = None,
        audio_features: Optional[torch.FloatTensor] = None,
        audio_features_lengths: Optional[torch.LongTensor] = None,
        batch_idx: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.LongTensor] = None,
    ):
        """ Extract speech codes by speech tokenizer, and reorg that in
            `input_ids`, `labels`
        """
        speech_codes, speech_codes_lens = self.speech_tokenizer.quantize(
            audio_features.transpose(1, 2), audio_features_lengths)
        for i in range(audio_features.size(0)):
            b = batch_idx[i]
            s, e = audio_offsets[i], audio_offsets[i] + speech_codes_lens[i]
            ids = speech_codes[
                i, :speech_codes_lens[i]] + self.speech_code_start_idx
            input_ids[b, s:e] = ids
            labels[b, s:e] = ids
        text_embs = self.llm.get_input_embeddings()(input_ids)
        if inputs_embeds is None:
            return text_embs, labels
        else:  # replace speech token emb
            for i in range(audio_features.size(0)):
                b = batch_idx[i]
                s, e = audio_offsets[i], audio_offsets[i] + speech_codes_lens[i]
                inputs_embeds[b, s:e] = text_embs[b, s:e]
            return inputs_embeds, labels

    @torch.autocast(device_type="cuda", dtype=torch.bfloat16)
    def forward(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.LongTensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        audio_offsets: Optional[torch.LongTensor] = None,
        audio_features: Optional[torch.FloatTensor] = None,
        audio_features_lengths: Optional[torch.LongTensor] = None,
        batch_idx: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
        **kwargs,
    ):
        inputs_embeds, labels = self.reorg_ids(input_ids, labels, audio_offsets,
                                               audio_features,
                                               audio_features_lengths,
                                               batch_idx, inputs_embeds)
        out = self.llm(inputs_embeds=inputs_embeds,
                       attention_mask=attention_mask,
                       labels=labels,
                       position_ids=position_ids,
                       **kwargs)
        return out

    @torch.autocast(device_type="cuda", dtype=torch.bfloat16)
    def generate(
        self,
        input_ids: torch.LongTensor = None,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.LongTensor] = None,
        audio_offsets: Optional[torch.LongTensor] = None,
        audio_features: Optional[torch.FloatTensor] = None,
        audio_features_lengths: Optional[torch.LongTensor] = None,
        batch_idx: Optional[torch.LongTensor] = None,
        text_lengths: Optional[torch.LongTensor] = None,
        inputs_embeds: Optional[torch.FloatTensor] = None,
    ):
        token_length = text_lengths[0].item()
        min_length = token_length * 2
        max_length = token_length * 20
        if inputs_embeds is None:
            inputs_embeds, labels = self.reorg_ids(input_ids, labels,
                                                   audio_offsets,
                                                   audio_features,
                                                   audio_features_lengths,
                                                   batch_idx)
        model_outputs = self.llm.generate(
            inputs_embeds=inputs_embeds,
            attention_mask=attention_mask,
            generation_config=self.generation_config,
            min_new_tokens=min_length,
            max_new_tokens=max_length,
        )
        return model_outputs

    def init_tokenizer(self):
        tokenizer = AutoTokenizer.from_pretrained(
            self.config.llm_model_name_or_path)
        tokenizer.bos_token = "<|im_start|>"
        return tokenizer
